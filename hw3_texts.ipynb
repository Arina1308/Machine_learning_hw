{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHgmxWG_7lnE"
   },
   "source": [
    "# Введение в анализ данных\n",
    "## НИУ ВШЭ, 2019-2020 учебный год\n",
    "\n",
    "### Домашнее задание №3\n",
    "\n",
    "Задание выполнил(а): Петросян Арина\n",
    "\n",
    "### Общая информация\n",
    "\n",
    "__Дата выдачи:__ 08.04.2020\n",
    "\n",
    "__Дедлайн:__ 23:59 22.04.2020\n",
    "\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Оценка за ДЗ вычисляется по следующей формуле:\n",
    "\n",
    "$$\n",
    "\\min(\\text{points}, 21)  \\times 10 / 21,\n",
    "$$\n",
    "\n",
    "где points — количество баллов за домашнее задание, которое вы набрали. Максимальное число баллов, которое можно получить за решение данного домашнего задания — 24, все баллы сверх 21 идут в бонус (таким образом, за данное домашнее задание можно получить 3 бонусных балла). Накопленные бонусные баллы можно будет потом распределять по другим домашним заданиям и проверочным (+1 бонусный балл = +1 к оценке за домашнее задание/проверочную).\n",
    "\n",
    "За сдачу задания позже срока на итоговую оценку за задание накладывается штраф в размере 1 балл в день, но получить отрицательную оценку нельзя.\n",
    "\n",
    "__Внимание!__ Домашнее задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов.\n",
    "\n",
    "### Формат сдачи\n",
    "\n",
    "Загрузка файлов с решениями происходит в системе [Anytask](https://anytask.org/).\n",
    "\n",
    "Инвайт для группы ИАД-4: zG1cIyT\n",
    "\n",
    "Перед отправкой перезагрузите ноутбук и проверьте, что все ячейки могут быть последовательно выполнены. Ноутбук должен запускаться с использованием python 3.6+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztx03xvr9T95"
   },
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVrrwTJNjuDt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 80.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# чтобы видеть проход по итерациям, можно использовать библиотеку tqdm\n",
    "# она работает примерно так:\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvXKae8q9nn-"
   },
   "source": [
    "### Данные\n",
    "\n",
    "Мы имеем дело с данными с торговой платформы Avito.\n",
    "Для каждого товара представлены следующие параметры:\n",
    " - `'title'`\n",
    " - `'description'`\n",
    " - `'Category_name'`\n",
    " - `'Category'`\n",
    "\n",
    "Имеется информация об объектах 50 классов.\n",
    "Задача: по новым объектам (`'title'`, `'description'`) предсказать `'Category'`.\n",
    "(Очевидно, что параметр `'Category_name'` для предсказания классов использовать нельзя)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "BqEuoDhqNgoa",
    "outputId": "b345f049-ae77-4d1b-a25f-4d4f447e63d2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>Category_name</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>382220</th>\n",
       "      <td>Прихожая</td>\n",
       "      <td>В хорошем состоянии. Торг</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397529</th>\n",
       "      <td>Кордиант 215/55/16 Летние</td>\n",
       "      <td>Кордиант 215/55/16 Летние/\\n /\\nАртикул: 1737l...</td>\n",
       "      <td>Запчасти и аксессуары</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584569</th>\n",
       "      <td>Стол</td>\n",
       "      <td>Стол, 2 рабочих места . Стол серого цвета, в д...</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513100</th>\n",
       "      <td>Комбинезон</td>\n",
       "      <td>Размер-42/44</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091886</th>\n",
       "      <td>Ветровка</td>\n",
       "      <td>На 2 года</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title  \\\n",
       "id                                   \n",
       "382220                    Прихожая   \n",
       "397529   Кордиант 215/55/16 Летние   \n",
       "584569                        Стол   \n",
       "2513100                 Комбинезон   \n",
       "1091886                   Ветровка   \n",
       "\n",
       "                                               description  \\\n",
       "id                                                           \n",
       "382220                           В хорошем состоянии. Торг   \n",
       "397529   Кордиант 215/55/16 Летние/\\n /\\nАртикул: 1737l...   \n",
       "584569   Стол, 2 рабочих места . Стол серого цвета, в д...   \n",
       "2513100                                       Размер-42/44   \n",
       "1091886                                          На 2 года   \n",
       "\n",
       "                     Category_name  Category  \n",
       "id                                            \n",
       "382220           Мебель и интерьер        20  \n",
       "397529       Запчасти и аксессуары        10  \n",
       "584569           Мебель и интерьер        20  \n",
       "2513100  Одежда, обувь, аксессуары        27  \n",
       "1091886     Детская одежда и обувь        29  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"avito_data.csv\", index_col='id')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Kg8iPp7fiwGh",
    "outputId": "96ed00ed-b63b-4478-f2d4-66bda1110b5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1hvzAMETU2d"
   },
   "outputs": [],
   "source": [
    "X = data[['title', 'description']].to_numpy()\n",
    "y = data['Category'].to_numpy()\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tMYU7zZw_cw-"
   },
   "source": [
    "Сразу разделим выборку на train и test.\n",
    "Никакие данные из test для обучения использовать нельзя!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fia4_3vNprp"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 27,  20,  84, 106,  27], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-ZEdlEGAXTD"
   },
   "source": [
    "### Токенизация (0.5 балла)\n",
    "\n",
    "\n",
    "Токенизация -- разбиение текста на мелкие части, которые можно обработать машинными методами.\n",
    "Можно использовать разные алгоритмы токенизации. В данном задании мы будем использовать `WordPunctTokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: Здраствуйте. Я, Кирилл. Хотел бы чтобы вы сделали игру, 3Д-экшон суть такова...\n",
      "after: ['здраствуйте', '.', 'я', ',', 'кирилл', '.', 'хотел', 'бы', 'чтобы', 'вы', 'сделали', 'игру', ',', '3д', '-', 'экшон', 'суть', 'такова', '...']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "text = 'Здраствуйте. Я, Кирилл. Хотел бы чтобы вы сделали игру, 3Д-экшон суть такова...'\n",
    "\n",
    "print(\"before:\", text,)\n",
    "print(\"after:\", tokenizer.tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_RYBKC26o1X"
   },
   "source": [
    "__Задание:__ реализуйте функцию ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "O9VgNlZ1Qy3o",
    "outputId": "59ef3a75-008e-47c5-fba8-a319eba13ef4"
   },
   "outputs": [],
   "source": [
    "def preprocess(text: str, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Данная функция принимает на вход текст, \n",
    "    а возвращает тот же текст, но с пробелами между каждым токеном\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    after_text = ' '.join(tokenizer.tokenize(text))\n",
    "    return after_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert preprocess(text, tokenizer) == 'здраствуйте . я , кирилл . хотел бы чтобы вы сделали игру , 3д - экшон суть такова ...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ токенизируйте `'title'` и `'description'` в `train` и `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z5WO-7tJUvbs"
   },
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    for j in range(len(X_train[i])):\n",
    "        X_train[i][j] = preprocess(X_train[i][j], tokenizer) \n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_test)):\n",
    "    for j in range((len(X_test[i]))):\n",
    "        X_test[i][j] = preprocess(X_test[i][j], tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDnDSWwFDwFo"
   },
   "outputs": [],
   "source": [
    "assert X_train[5][0] == '1 - к квартира , 33 м² , 4 / 5 эт .'\n",
    "assert X_train[10][1] == 'продам иж планета 3 , 76 год , ( стоит на старом учёте , документы утеряны ) на ходу , хорошее состояние , все интересующие вопросы по телефону ( с родной коляской на 3 тысячи дороже ) . торга не будет .'\n",
    "assert X_test[2][0] == 'фара правая toyota rav 4 галоген 2015 - 19'\n",
    "assert X_test[2][1] == 'фара правая для toyota rav4 2015 / оригинальный номер : 8113042650 / тойота рав4 тоета рав 4 / производитель : toyota / состояние : отличное без дефектов ! / комментарий : после 2015 не ксенон галоген + диод / пожалуйста , уточняйте соответствие вашего заказа изображенному на фото . / звоните уточняйте по наличию предоставляется время на проверку детали / отправляем в регионы рф транспортными компаниями / . / всегда включен вайбер вацап по вопросам !/ дополнительное фото по запросу'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlIITUk0AsmS"
   },
   "source": [
    "### BOW (3 балла)\n",
    "\n",
    "Один из традиционных подходов -- построение bag of words.\n",
    "\n",
    "Метод состоит в следующем:\n",
    "\n",
    " - Составить словарь самых часто встречающихся слов в `train data`\n",
    " - Для каждого примера из `train` посчитать, сколько раз каждое слово из словаря в нём встречается\n",
    "\n",
    "\n",
    " В `sklearn` есть `CountVectorizer`, но в этом задании его использовать нельзя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMKUttDWIF92"
   },
   "source": [
    "__Задание:__ создайте словарь, где каждому токену соответствует количество раз, которое оно встретилось в `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_counter(items):\n",
    "    cnt = {}\n",
    "    for lst in items:\n",
    "        for text in lst:\n",
    "            for w in text.split():\n",
    "                if w in cnt:\n",
    "                    cnt[w] += 1\n",
    "                else:\n",
    "                    cnt[w] = 1\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_cnt = tokens_counter(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokens_cnt['сапоги'] == 454"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ выведите 10 самых частотных и 10 самых редких токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_cnt_list = list(tokens_cnt.items())\n",
    "tokens_cnt_list.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent = []\n",
    "for i in tokens_cnt_list[:10]:\n",
    "    most_frequent.append(i[0])\n",
    "    \n",
    "least_frequent = []\n",
    "for i in tokens_cnt_list[-10:]:\n",
    "    least_frequent.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 самых частотных токенов:\n",
      "['/', ',', '.', '-', 'в', 'и', 'на', './', ':', 'с']\n",
      "10 самых редких токенов:\n",
      "['шуршат', 'гремят', 'петровского', 'столиц', 'объективную', 'понравившейся', 'беспрецедентно', 'дооснастить', 'хлебозаводская', 'фрионом']\n"
     ]
    }
   ],
   "source": [
    "print('10 самых частотных токенов:', most_frequent, sep='\\n') # your code here\n",
    "print('10 самых редких токенов:', least_frequent, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ оставьте в словаре только топ-10000 самых частотных токенов, также создайте отдельный список из этих слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_cnt = dict(tokens_cnt_list[:10000]) # your code here\n",
    "tokens_list = list(tokens_cnt.keys()) # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9627"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_cnt['для']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ реализуйте функцию, которая переводит текст в вектор из чисел. То есть каждому токену из списка токенов сопоставляется количество раз, которое он встретился в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4awkhecbR9om"
   },
   "outputs": [],
   "source": [
    "def text_to_bow(text: str, tokens_list: list) -> np.array:\n",
    "    \"\"\"\n",
    "    Возвращает вектор, где для каждого слова из словаря\n",
    "    указано количество его употреблений в предложении\n",
    "    input: строка, список токенов\n",
    "    output: вектор той же размерности, что и список токенов\n",
    "    \"\"\"\n",
    "    \n",
    "    a = np.array(tokens_list)\n",
    "    tokens_dict = dict(zip(a, np.zeros(a.shape)))\n",
    "    for word in text.split():\n",
    "        if word in tokens_list:\n",
    "            tokens_dict[word] += 1\n",
    "    return np.array(list(tokens_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = text_to_bow(\"сдаётся уютный , тёплый гараж для стартапов в ml\", tokens_list)\n",
    "\n",
    "assert np.allclose(example_text.mean(), 0.0008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,    4,   12,  565,  866, 1601, 2539, 4063], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(example_text)[0] # как и у многих, assert ломается из-за разной индексации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ а теперь реализуйте функцию, которая преобразует наш датасет и каждому тексту из `'description'` сопоставляет вектор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HR_D8Fn4pudv"
   },
   "outputs": [],
   "source": [
    "def descr_to_bow(items: np.array, tokens_list: list) -> np.array:\n",
    "    \"\"\" Для каждого описания товара возвращает вектор его bow \"\"\"\n",
    "    \n",
    "    all_texts_vec = np.zeros((len(items), len(tokens_list)))\n",
    "    for i in tqdm(range(len(items))):\n",
    "        all_texts_vec[i] = text_to_bow(items[i][1], tokens_list)\n",
    "    return all_texts_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "wwOZaEpMSQsZ",
    "outputId": "8a30c3af-3517-42bd-a5f3-36206b4b264a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 21000/21000 [10:33<00:00, 33.30it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_bow = descr_to_bow(X_train, tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 9000/9000 [04:27<00:00, 33.63it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test_bow = descr_to_bow(X_test, tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 10000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.33236433289665"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(X_train_bow[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train_bow.shape == (21000, 10000), X_test_bow.shape == (9000, 10000)\n",
    "assert 0.005 < X_train_bow.mean() < 0.006\n",
    "assert 0.005 < X_test_bow.mean() < 0.006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJoXiCWI7VF5"
   },
   "source": [
    "### Логистическая регрессия и SVC (0.5 балла)\n",
    "\n",
    "\n",
    "Теперь описание каждого товара представлено, как точка в многомерном пространстве.\n",
    "Очень важно запомнить эту идею: дальше мы будем рассматривать разные способы перехода от текста к точке в пространстве.\n",
    "\n",
    "Для BOW каждое измерение в пространстве -- какое-то слово.\n",
    "Мы предполагаем, что текст описывается набором каких-то популярных слов, которые в нём встречаются, а близкие по смыслу тексты будут использовать одинаковые слова.\n",
    "\n",
    "Обучите логистическую регрессию и SVM с линейным ядром (`sklearn.svm.LinearSVC` или `sklearn.svm.SVC(kernel='linear')`) с базовыми параметрами. При необходимости можете увеличить максимальное число итераций. В качестве `random_state` возьмите 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Подсказка: для того, чтобы было проще обучать, можно использовать [разреженные матрицы](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D0%B7%D1%80%D0%B5%D0%B6%D0%B5%D0%BD%D0%BD%D0%B0%D1%8F_%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D0%B0) - многие модели из `sklearn` умеют с ними работать. Соответствующий модуль из `scipy`: [scipy.sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html). Нетрудно заметить, что в полученных BOW-матрицах очень много нулей. Если хранить в памяти только ненулевые элементы, можно сильно оптимизировать вычисления. Можете в этом убедиться:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train array in memory (raw): 1680.000 Mb\n",
      "Train array in memory (compressed): 8.606 Mb\n"
     ]
    }
   ],
   "source": [
    "print('Train array in memory (raw): {:.3f} Mb'.format(X_train_bow.nbytes * 1e-6))\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "X_train_bow_csr = csr_matrix(X_train_bow)\n",
    "print('Train array in memory (compressed): {:.3f} Mb'.format(\n",
    "    (X_train_bow_csr.data.nbytes + X_train_bow_csr.indptr.nbytes + X_train_bow_csr.indices.nbytes) * 1e-6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJVLS8Fs3CeT"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(random_state=13)\n",
    "lr = lr.fit(X_train_bow_csr, y_train) # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "colab_type": "code",
    "id": "Ky3HV1rTSS9L",
    "outputId": "612a5f0d-76bd-44f4-eeeb-63b517443797"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7046666666666667"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_lr = lr.predict(X_test_bow) # predict\n",
    "\n",
    "assert accuracy_score(y_test, y_pred_lr) > 0.695\n",
    "accuracy_score(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "SVM = LinearSVC(random_state=13)\n",
    "SVM = SVM.fit(X_train_bow_csr, y_train) # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "-c46ZT0lvF6T",
    "outputId": "4b1cb34a-201b-4dc2-9155-fdb6919c6c08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6842222222222222"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_svm = SVM.predict(X_test_bow)\n",
    "\n",
    "assert accuracy_score(y_test, y_pred_svm) > 0.68\n",
    "accuracy_score(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21000, 10000), (21000,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow_csr.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwKE57YZ1Hzn"
   },
   "source": [
    "### Модификация признаков (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ewMlxQezL6Ax"
   },
   "source": [
    "Прибавьте к соответствующим BOW-векторам BOW-вектора для `'title'` товара с некоторым весом. Изменится ли качество? Как вы можете это объяснить?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_to_bow(items: np.array, tokens_list: list) -> np.array:\n",
    "    \"\"\" Для каждого названия товара возвращает вектор его bow \"\"\"\n",
    "    \n",
    "    all_texts_vec = np.zeros((len(items), len(tokens_list)))\n",
    "    for i in tqdm(range(len(items))):\n",
    "        all_texts_vec[i] = text_to_bow(items[i][0], tokens_list)\n",
    "    return all_texts_vec\n",
    " # your code here for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 21000/21000 [09:05<00:00, 38.53it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_bow_t = title_to_bow(X_train, tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21000, 10000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow_td = X_train_bow_t + X_train_bow\n",
    "X_train_bow_td.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 9000/9000 [03:55<00:00, 34.03it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test_bow_t = title_to_bow(X_test, tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 10000)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_bow_td = csr_matrix(X_test_bow_t + X_test_bow)\n",
    "X_test_bow_td.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = lr.fit(X_train_bow_td, y_train) # лог регрессия на ненормализованных данных\n",
    "y_pred_td_lr = lr.predict(X_test_bow_td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7837777777777778"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_td_lr) # для лог регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "SVM = SVM.fit(X_train_bow_td, y_train)  # SVM на ненормализованных данных\n",
    "y_pred_td_svm = SVM.predict(X_test_bow_td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7535555555555555"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_td_svm) # для SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Как видим качество возросло, так как добавление title помогает получить больше информации о товаре"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Db4TyqzxMnby"
   },
   "source": [
    "Нормализуйте данные с помощью `MinMaxScaler` или `MinAbsScaler` перед обучением. Что станет с качеством и почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A8rVy6q1Mn4J"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler # your code here for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MaxAbsScaler() # нормализуем признаки\n",
    "X_train_bow_norm = sc.fit_transform(X_train_bow_td)\n",
    "X_test_bow_norm = sc.transform(X_test_bow_td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = lr.fit(X_train_bow_norm, y_train) # лог регрессия на нормализованных данных\n",
    "y_pred_n_lr = lr.predict(X_test_bow_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7315555555555555"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_n_lr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = SVM.fit(X_train_bow_norm, y_train)  # SVM на нормализованных данных\n",
    "y_pred_n_svm = SVM.predict(X_test_bow_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7593333333333333"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_n_svm) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- заметим, что относительно первоначального алгоритма качество выросло, однако оно хуже, чем до нормализации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему в данном случае использовать `StandardScaler` - не очень хорошая идея?\n",
    "    \n",
    "Ответ: потому что приведение к нормальному стндартному распрделению лучше всего использовать тогда, когда признаки изначально имеют распределени, близкое к нормальному. Также StandardScaler при нормализации делит на стандартное отклонение, что в данном случае не лучшкее решение, тк в векторах очень много нулей и  дисперсия нередко будет стремиься к нулю."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvCAL3qGDByj"
   },
   "source": [
    "### Иная предобработка (1 балл)\n",
    "\n",
    "**На выбор**:\n",
    "\n",
    "- **либо** обучите модели, используя для предобработки токенизатор и лемматизатор `pymystem3.Mystem`.\n",
    "- **либо** добавьте к предобработке стэмминг.\n",
    "\n",
    "Сравните полученное сейчас качество с полученным ранее и сделайте вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGvNHfVsDfhq"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    stemmed = [stemmer.stem(w) for w in text.split()]\n",
    "    return ' '.join(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z5WO-7tJUvbs"
   },
   "outputs": [],
   "source": [
    "X_train_stemmed = np.zeros_like(X_train)\n",
    "for i in range(len(X_train)):\n",
    "    for j in range(len(X_train[i])):\n",
    "        X_train_stemmed[i][j] = stem_text(X_train[i][j]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z5WO-7tJUvbs"
   },
   "outputs": [],
   "source": [
    "X_test_stemmed = np.zeros_like(X_test)\n",
    "for i in range(len(X_test)):\n",
    "    for j in range(len(X_test[i])):\n",
    "        X_test_stemmed[i][j] = stem_text(X_test[i][j]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_tokens = tokens_counter(X_train_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_list = list(stemmed_tokens.items())\n",
    "stemmed_list.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_tokens = dict(stemmed_list[:10000]) \n",
    "stemmed_list = list(stemmed_tokens.keys()) # нашли 10000 самых частых токенов  после стэмминга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 21000/21000 [10:13<00:00, 34.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 9000/9000 [04:33<00:00, 32.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 21000/21000 [10:08<00:00, 24.66it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 9000/9000 [06:38<00:00, 39.10it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_stemmed_bow_descr = descr_to_bow(X_train_stemmed, stemmed_list)\n",
    "X_test_stemmed_bow_descr = descr_to_bow(X_test_stemmed, stemmed_list)\n",
    "X_train_stemmed_bow_title = title_to_bow(X_train_stemmed, stemmed_list)\n",
    "X_test_stemmed_bow_title = title_to_bow(X_test_stemmed, stemmed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stemmed_bow = X_train_stemmed_bow_descr + X_train_stemmed_bow_title\n",
    "X_test_stemmed_bow = X_test_stemmed_bow_descr + X_test_stemmed_bow_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = lr.fit(X_train_stemmed_bow, y_train) # лог регрессия\n",
    "y_pred_st_lr = lr.predict(X_test_stemmed_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8034444444444444"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_st_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "SVM = SVM.fit(X_train_stemmed_bow, y_train) # SVM\n",
    "y_pred_st_svm = SVM.predict(X_test_stemmed_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7736666666666666"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_st_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ВЫВОД: Стемминг достаточно сильно повышает качество, несмотря на то что он даже не приводит к начальной форме, а только отбрасывает окончания и суфиксы (префиксы)!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXbsPtpfoB7m"
   },
   "source": [
    "### TF-IDF (5 баллов)\n",
    "\n",
    "Не все слова полезны одинаково, давайте попробуем [взвесить](http://tfidf.com/) их, чтобы отобрать более полезные.\n",
    "\n",
    "\n",
    "> TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "> \n",
    "> IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
    "\n",
    "\n",
    "В `sklearn` есть `TfidfVectorizer`, но в этом задании его использовать нельзя. Для простоты посчитайте общий tf-idf для `'title'` и `'description'` (то есть каждому объекту надо сопоставить вектор, где как документ будет рассматриваться конкатенация `'title'` и `'description'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ составьте словарь, где каждому слову из изначального списка будет соответствовать количество документов из `train`-части, где это слово встретилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_doc = []\n",
    "for i in range(21000):\n",
    "    X_train_doc.append(' '.join(X_train[i]))\n",
    "X_train_doc = np.array(X_train_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_doc = []\n",
    "for i in range(9000):\n",
    "    X_test_doc.append(' '.join(X_test[i]))\n",
    "X_test_doc = np.array(X_test_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_document_cnt = dict(zip(tokens_list, np.zeros(10000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 21000/21000 [00:02<00:00, 10150.82it/s]\n"
     ]
    }
   ],
   "source": [
    "for document in tqdm(X_train_doc):\n",
    "    for word in set(document.split()):\n",
    "        if word in word_document_cnt:\n",
    "            word_document_cnt[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert word_document_cnt['размер'] == 2839"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ реализуйте функцию, где тексту в соответствие ставится tf-idf вектор. Для вычисления IDF также необходимо число документов в `train`-части (параметр `n_documents_total`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6i5zFpD9rbtz"
   },
   "outputs": [],
   "source": [
    "def text_to_tfidf(text: str, word_document_cnt: dict, tokens_list: list, n_documents_total: int) -> np.array:\n",
    "    \"\"\"\n",
    "    Возвращает вектор, где для каждого слова из словаря\n",
    "    указан tf-idf\n",
    "    \"\"\"\n",
    "    text_words = text.split()\n",
    "    l = len(text_words)\n",
    "    tf_idf = dict(zip(word_document_cnt, np.zeros(10000)))\n",
    "    for word in set(text_words):\n",
    "        if word in word_document_cnt:\n",
    "            tf = text_words.count(word) / l\n",
    "            idf = np.log(n_documents_total / word_document_cnt[word])\n",
    "            tf_idf[word] = tf*idf\n",
    "    return np.array(list(tf_idf.values()))\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = text_to_tfidf(\n",
    "    'сдаётся уютный , тёплый гараж для стартапов в ml',\n",
    "    word_document_cnt,\n",
    "    tokens_list,\n",
    "    n_documents_total=len(X_train)\n",
    ")\n",
    "assert np.allclose(np.linalg.norm(example_text), 1.4435668)\n",
    "assert 0.0003 < example_text.mean() < 0.0004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ а теперь реализуйте функцию, которая преобразует наш датасет и для каждого объекта сопоставляет вектор tf-idf. В качестве текстов используйте конкатенацию `'title'` и `'description'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def items_to_tfidf(items: np.array, word_document_cnt: dict, tokens_list: list, n_documents_total: int) -> np.array:\n",
    "    \"\"\"\n",
    "    Для каждого товара возвращает его tf-idf вектор\n",
    "    \"\"\"\n",
    "    all_texts_vec = np.zeros((len(items), 10000))\n",
    "    for i in tqdm(range(len(items))):\n",
    "        all_texts_vec[i] = text_to_tfidf(items[i], word_document_cnt, tokens_list, n_documents_total)\n",
    "    return all_texts_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 21000/21000 [02:27<00:00, 142.73it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf = items_to_tfidf(X_train_doc, word_document_cnt, tokens_list, len(X_train_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 9000/9000 [01:02<00:00, 145.11it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test_tfidf = items_to_tfidf(X_test_doc, word_document_cnt, tokens_list, len(X_train_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21000, 10000)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train_tfidf.shape == (21000, 10000), X_test_tfidf.shape == (9000, 10000)\n",
    "assert 0.0002 < X_train_tfidf.mean() < 0.0004\n",
    "assert 0.0002 < X_test_tfidf.mean() < 0.0004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-d167608ce739>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.32672414\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# старый assert\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_tfidf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.37996078\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert np.allclose(np.linalg.norm(X_train_tfidf[-3]), 0.32672414) # старый assert\n",
    "assert np.allclose(np.linalg.norm(X_test_tfidf[0]), 0.37996078)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3278140104037421, 0.3799607690718185)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(X_train_tfidf[-3]), np.linalg.norm(X_test_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-YFA-8kE1RHk"
   },
   "source": [
    "__Задание:__ обучите логистическую регрессию и SVC, оцените качество (accuracy_score). Сделайте вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отнормируем данные\n",
    "X_train_tfidf_n = sc.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf_n = sc.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим разреженные матрицы\n",
    "X_train_tfidf_csr = csr_matrix(X_train_tfidf_n)\n",
    "X_test_tfidf_csr = csr_matrix(X_test_tfidf_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ULrXsF1m5sU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# your code here for LogReg\n",
    "lr_model = LogisticRegression(random_state=13)\n",
    "lr_model = lr_model.fit(X_train_tfidf_csr, y_train)\n",
    "\n",
    "assert accuracy_score(y_test, lr_model.predict(X_test_tfidf_csr)) > 0.675"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7167777777777777"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, lr_model.predict(X_test_tfidf_csr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here for SVM (SVM без нормировки)\n",
    "svc_model = LinearSVC(random_state=13)\n",
    "svc_model = svc_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "assert accuracy_score(y_test, svc_model.predict(X_test_tfidf)) > 0.79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7942222222222223"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, svc_model.predict(X_test_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ВЫВОД (о TF-IDF): Нормализация помогает улучшить качество при использовании Логистической регрессии, но не SVC. Также TF-IDF в принципе работает лучше с SVC, для лог. регрессии метод bow-векторов подошел больше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vQZ61xSsTpZI"
   },
   "source": [
    "### Word Vectors (4 балла)\n",
    "\n",
    "Давайте попробуем другой подход -- каждому слову сопоставим какое-то векторное представление (эмбеддинг) - но достаточно маленькой размерности. Таким образом мы сильно уменьшим количество параметров в модели.\n",
    "\n",
    "Почитать про это подробнее можно тут:\n",
    "\n",
    "- https://habr.com/ru/company/ods/blog/329410/\n",
    "\n",
    "Вектора мы возьмём уже готовые (обученные на текстах из интернета), так что наша модель будет знать некоторую дополнительную информацию о внешнем мире."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zfse4xVbgMIr"
   },
   "outputs": [],
   "source": [
    "!tar -xzf ru.tar.gz\n",
    "# распаковка файла - опять же, если не работает, распакуйте вручную"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\arina\\anaconda3\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\arina\\anaconda3\\lib\\site-packages (from gensim) (1.16.2)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\arina\\anaconda3\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\arina\\anaconda3\\lib\\site-packages (from gensim) (1.11.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\arina\\anaconda3\\lib\\site-packages (from gensim) (1.2.1)\n",
      "Requirement already satisfied: boto in c:\\users\\arina\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in c:\\users\\arina\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.21.0)\n",
      "Requirement already satisfied: boto3 in c:\\users\\arina\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.12.41)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\arina\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arina\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\arina\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\arina\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\arina\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.41 in c:\\users\\arina\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (1.15.41)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\arina\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\arina\\anaconda3\\lib\\site-packages (from botocore<1.16.0,>=1.15.41->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\arina\\anaconda3\\lib\\site-packages (from botocore<1.16.0,>=1.15.41->boto3->smart-open>=1.8.1->gensim) (0.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sy2TXmQ2jZSY"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "embedding_model = FastText.load_fasttext_format('ru.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[ 0.02916384  0.02167605  0.05127367 -0.00971958  0.0465235  -0.03945766\n",
      "  0.02737866  0.00638128 -0.03774629 -0.04257201 -0.00995653  0.02291315\n",
      " -0.02301722  0.06697998 -0.03674482 -0.02403202 -0.05404469  0.01372932\n",
      "  0.00926399 -0.0013149   0.11941359 -0.022448    0.04011497  0.06980549\n",
      "  0.00407011 -0.09384539  0.03050164 -0.02578281 -0.03525181 -0.06603175\n",
      "  0.04752798  0.05874675  0.01983666  0.06092105 -0.00957561  0.08307806\n",
      " -0.01288903  0.04705157  0.02198839 -0.00649013 -0.0171444   0.03302203\n",
      "  0.02124882 -0.01902875 -0.05235172  0.03458685 -0.01409259 -0.07477519\n",
      "  0.01916078  0.02985001  0.0086322   0.03051201  0.02831862  0.04549561\n",
      "  0.00761138 -0.05459622  0.09056009 -0.08807947 -0.05420396 -0.04793203\n",
      " -0.05672329 -0.03025264 -0.03024072 -0.05890108 -0.03137474  0.03292617\n",
      "  0.05440779 -0.04548327 -0.07266086 -0.09327219  0.07247883  0.0111061\n",
      "  0.01824225 -0.10570452  0.05110046 -0.04659343 -0.03277056 -0.00803401\n",
      " -0.03978698  0.00826598 -0.01074128  0.018431   -0.10150263 -0.00472604\n",
      "  0.06706332  0.02466901  0.09045192 -0.05226929  0.04866098 -0.02843297\n",
      "  0.04756537  0.00261342  0.06845197  0.00082511 -0.00547984  0.0100649\n",
      "  0.02135489 -0.01437242  0.00191435  0.11989547  0.02357679  0.07061605\n",
      "  0.03375214  0.05462346  0.08270866  0.00126649  0.03054527  0.04314573\n",
      " -0.00719835 -0.02799017  0.00249404  0.00139046 -0.04099929  0.00526204\n",
      "  0.01386764  0.02106066  0.00887202  0.05943111 -0.07185322  0.03263306\n",
      "  0.00284878  0.03816929  0.0210096  -0.030828    0.00502779  0.09250114\n",
      "  0.02399154  0.05744717 -0.04319151  0.04075926 -0.03877947  0.0605263\n",
      " -0.00837917 -0.04922852 -0.04570796  0.02973622 -0.01798053  0.00413011\n",
      " -0.00712464 -0.01312802  0.05847022 -0.07881333 -0.02204878  0.03086594\n",
      "  0.02965177 -0.0073295  -0.02443145 -0.06222062  0.01083152  0.06009534\n",
      " -0.02042049  0.06301811  0.02287635 -0.03021961  0.04831248  0.02882019\n",
      "  0.04446645 -0.01677353 -0.08272323 -0.06830658  0.08947854  0.03370909\n",
      " -0.00895046 -0.00681254 -0.02059644 -0.09527113  0.02611189 -0.06112244\n",
      "  0.01080315  0.01901113  0.00810233  0.00742132  0.10493557 -0.00522375\n",
      "  0.05826566  0.03236291  0.03787734 -0.05026894 -0.08401242  0.02860721\n",
      " -0.05106218  0.02631241  0.02631763  0.06924202  0.03319636  0.00980412\n",
      "  0.04016861  0.03428936  0.00652957 -0.01058654 -0.0245588   0.1464914\n",
      " -0.01041028  0.03553488 -0.07482928 -0.01063148 -0.0342233  -0.01662586\n",
      " -0.00029508  0.04694034 -0.00062491 -0.0435293  -0.01315623  0.07061336\n",
      "  0.01603698  0.02374655  0.05453315  0.00253603 -0.0313729  -0.02740866\n",
      "  0.04278845 -0.00810288  0.03973977  0.07674816  0.04658518 -0.02685211\n",
      " -0.05009724  0.0060723  -0.04231661  0.02584185 -0.03419575 -0.03799306\n",
      "  0.06701688 -0.1245426   0.03846397 -0.0855662  -0.01193651  0.04968415\n",
      "  0.03559558  0.10029506  0.05714916  0.01145345 -0.03564315 -0.00924199\n",
      "  0.08630151  0.08049053  0.05822275 -0.05224873 -0.02462301  0.05832206\n",
      " -0.04124978  0.00186134  0.00782246  0.01179015 -0.02291097  0.00614069\n",
      "  0.01782681  0.02190027  0.04341367  0.06151633 -0.01183114 -0.00141502\n",
      "  0.06193598  0.0611085  -0.02373199 -0.05797793 -0.02269631  0.11511736\n",
      " -0.04581353 -0.05082048 -0.04706197  0.0429772   0.00409648 -0.0141248\n",
      "  0.01417164  0.00575812 -0.07616108 -0.01051838  0.05149659  0.02367133\n",
      "  0.00073724  0.05957585 -0.11871962  0.03876314  0.03472188 -0.02344368\n",
      " -0.01165281 -0.01397923  0.08815268  0.03459521  0.07113555 -0.03984846\n",
      " -0.01600395  0.01932258  0.01351069 -0.06409036 -0.02024848  0.05895981\n",
      "  0.02591374 -0.04027611  0.00654722  0.05093394 -0.02461737  0.02561689\n",
      " -0.01412898 -0.00366109 -0.06719207  0.00742674 -0.02095614 -0.06263787]\n"
     ]
    }
   ],
   "source": [
    "# как мы видим, каждому слову данная модель сопоставляет вектор размерности 300\n",
    "\n",
    "print(embedding_model['привет'].shape)\n",
    "print(embedding_model['привет'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_model['размер'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ реализуйте функцию, выдающую эмбеддинг для предложения - как сумму эмбеддингов токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H49QR_jhjmCa"
   },
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence: str, embedding_model) -> np.array:\n",
    "    \"\"\"\n",
    "    Складывает вектора токенов строки sentence\n",
    "    \"\"\"\n",
    "    embed = np.zeros(len(embedding_model['размер']))\n",
    "    for word in sentence.split():\n",
    "        if word in embedding_model:\n",
    "            embed += embedding_model[word]\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gj6U_hjtlllV"
   },
   "outputs": [],
   "source": [
    "assert sentence_embedding('сдаётся уютный , тёплый гараж для стартапов в ml', embedding_model).shape == (300,)\n",
    "assert np.allclose(np.linalg.norm(sentence_embedding('сдаётся уютный , тёплый гараж для стартапов в ml', embedding_model)), 2.6764746)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.676474469401541"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(sentence_embedding('сдаётся уютный , тёплый гараж для стартапов в ml', embedding_model)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ сделайте все то же, что в предыдущих пунктах -- реализуйте функцию, которая преобразует данные, а затем обучите логистическую регрессию и SVM, оцените качество. Сделайте вывод, что работает лучше - модель, основанная на TF-IDF, или модель, обученная на предобученных эмбеддингах?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8tfhc-PFmGvu"
   },
   "outputs": [],
   "source": [
    "def items_embedding(items: np.array, embedding_model):\n",
    "    all_texts_vec = np.zeros((len(items), len(embedding_model['размер'])))\n",
    "    for i in tqdm(range(len(items))):\n",
    "        all_texts_vec[i] = sentence_embedding(items[i], embedding_model)\n",
    "    return all_texts_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 21000/21000 [00:51<00:00, 407.57it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 9000/9000 [00:22<00:00, 400.29it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_embedding = items_embedding(X_train_doc, embedding_model)\n",
    "X_test_embedding = items_embedding(X_test_doc, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5782222222222222"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = lr_model.fit(X_train_embedding, y_train)\n",
    "y_pred_emb_lr = lr_model.predict(X_test_embedding)\n",
    "\n",
    "accuracy_score(y_test, y_pred_emb_lr) # как же сильно упало качество..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5747777777777778"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_model = svc_model.fit(X_train_embedding, y_train)\n",
    "y_pred_emb_svc = svc_model.predict(X_test_embedding)\n",
    "\n",
    "accuracy_score(y_test, y_pred_emb_svc) # просто ужас("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробуем отнормировать\n",
    "X_train_embedding = sc.fit_transform(X_train_embedding)\n",
    "X_test_embedding = sc.transform(X_test_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3641111111111111"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = lr_model.fit(X_train_embedding, y_train)\n",
    "y_pred_emb_lr_n = lr_model.predict(X_test_embedding)\n",
    "\n",
    "accuracy_score(y_test, y_pred_emb_lr_n) # еще хуже, не стоило так делать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_model = svc_model.fit(X_train_embedding, y_train)\n",
    "y_pred_emb_svc_n = svc_model.predict(X_test_embedding)\n",
    "\n",
    "accuracy_score(y_test, y_pred_emb_svc_n) # мда..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ВЫВОД: полагаю, эмбеддинг показал такие плохие результаты, потому что токены не лемматизированы и вообще embedding_model включает в себя рандомные слова. Таким образом, в данном случае TF-IDF работате лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cVEdlFostSnX"
   },
   "source": [
    "### Что дальше? (8 баллов)\n",
    "\n",
    "Для получения максимальной оценки вам нужно решить любые 2 пункта.\n",
    "\n",
    "1. Реализовать n-gram модели текстовой классификации (__2 балла__)\n",
    "\n",
    "2. Поработать с другими эмбеддингами для слов (например `word2vec` или `GloVe`) (__2 балла__)\n",
    "\n",
    "3. Применить другие способы токенизации (например, `pymorphy2`, `spaCy`) и в целом предобработки данных (стоп-слова, стэмминг, лемматизация) (__2 балла__)\n",
    "\n",
    "4. Добиться качества > 0.82 на тестовых данных (попробуйте другие токенизаторы, предобработку текста, и любые другие идеи, которые вам придут в голову) (__2 балла__)\n",
    "\n",
    "Снабжайте код пояснениями и графиками.\n",
    "Обязательно необходимо написать вывод по каждому пункту, который вы реализуете."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Реализуем n-gram модели!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 3)) # обозначим функцию, преобразующую в необходимые векторы наш массив\n",
    "bow = vec.fit_transform(X_train_doc) # применим ее\n",
    "\n",
    "# обучим Лог. регрессию\n",
    "lr_model.fit(bow, y_train)\n",
    "y_pred_ngram_lr = lr_model.predict(vec.transform(X_test_doc)) # заметила, что с увеличением n точность падает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7736666666666666"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_ngram_lr) # на триграммах точность была 0.47, а тут выросла!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model.fit(bow, y_train)\n",
    "y_pred_ngram_svc = svc_model.predict(vec.transform(X_test_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7875555555555556"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_ngram_svc) # тут аналогичная ситуация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Построим график дя n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [03:59<00:59, 59.03s/it]C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [05:59<00:00, 77.28s/it]\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "iii = []\n",
    "for i in tqdm(range(1, 6)):\n",
    "    vec = CountVectorizer(ngram_range=(i, i)) # обозначим функцию, преобразующую в необходимые векторы наш массив\n",
    "    X_train_b = vec.fit_transform(X_train_doc)\n",
    "    svc_model.fit(X_train_b, y_train)\n",
    "    y_pred_ngram_i = svc_model.predict(vec.transform(X_test_doc))\n",
    "    accuracy.append(accuracy_score(y_test, y_pred_ngram_i))\n",
    "    iii.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy score')"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(9, 6))\n",
    "plt.plot(iii, accuracy, '--go', c = 'r', ms=15, alpha=0.5)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(0.5, 5.5)\n",
    "plt.grid(color='grey', linestyle='-', linewidth=0.2)\n",
    "plt.title('Изменение качества алгоритма в зависимости от n')\n",
    "plt.xlabel('n in n-grams')\n",
    "plt.ylabel('Accuracy score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ВЫВОД: при использовании n-gram стоит поэспериментировать с гипер-параметрами, возможно, даже стоило grid_search запустить. Но ручным перебором заметила, что с уменьшением n точность возрастала. Самыми оптимальным оказались униграммы, но чтобы разнообразить домашку,  использовала range от униграмм до триграмм (1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Поработаем с word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.sklearn_api import D2VTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "path = get_tmpfile(\"word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим векторы для слов из выборки\n",
    "model = Word2Vec([i.split() for i in X_train_doc], size=250, min_count=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = model.wv # присвоим название для удобства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 21000/21000 [00:05<00:00, 3955.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 9000/9000 [00:02<00:00, 3761.83it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_wv_embedding = items_embedding(X_train_doc, wv_embedding)\n",
    "X_test_wv_embedding = items_embedding(X_test_doc, wv_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5548888888888889"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = lr_model.fit(X_train_wv_embedding, y_train)\n",
    "y_pred_wv_lr = lr_model.predict(X_test_wv_embedding)\n",
    "\n",
    "accuracy_score(y_test, y_pred_wv_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4275555555555556"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_model = svc_model.fit(X_train_embedding, y_train)\n",
    "y_pred_emb_svc = svc_model.predict(X_test_embedding)\n",
    "\n",
    "accuracy_score(y_test, y_pred_emb_svc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ВЫВОД: word2vec, создающий эмбеддинги на основе нашей выборки, еще менее эффективен, чем когда нам что-то известно о внешнем мире"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Другая предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
      "Collecting pymorphy2-dicts<3.0,>=2.4 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
      "Collecting docopt>=0.6 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
      "Collecting dawg-python>=0.7 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\arina\\AppData\\Local\\pip\\Cache\\wheels\\9b\\04\\dd\\7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\n",
      "Successfully built docopt\n",
      "Installing collected packages: pymorphy2-dicts, docopt, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n",
      "Requirement already satisfied: pymorphy2-dicts in c:\\users\\arina\\anaconda3\\lib\\site-packages (2.4.393442.3710985)\n",
      "Requirement already satisfied: DAWG-Python in c:\\users\\arina\\anaconda3\\lib\\site-packages (0.7.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2\n",
    "!pip install pymorphy2-dicts\n",
    "!pip install DAWG-Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "pymorphy2_analyzer = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify(items: np.array, pymorphy2_analyzer):\n",
    "    '''\n",
    "    Обрабатывает каждый документ в X и возвращает в новый массив\n",
    "    '''\n",
    "    X_pym = []\n",
    "    for i in tqdm(range(len(items))):\n",
    "        X = []\n",
    "        for word in items[i].split():\n",
    "            a = pymorphy2_analyzer.parse(word)\n",
    "            X.append(a[0].normal_form)\n",
    "        X_pym.append(' '.join(X))\n",
    "    return np.array(X_pym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 21000/21000 [03:20<00:00, 104.94it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_pym = modify(X_train_doc, pymorphy2_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 9000/9000 [01:27<00:00, 103.32it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test_pym = modify(X_test_doc, pymorphy2_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer()\n",
    "X_train_pym_bow = vec.fit_transform(X_train_pym)\n",
    "X_test_pym_bow = vec.transform(X_test_pym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\arina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr_model.fit(X_train_pym_bow, y_train)\n",
    "y_pred_pym_lr = lr_model.predict(X_test_pym_bow) # Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.806"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_pym_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = svc_model.fit(X_train_pym_bow, y_train)\n",
    "y_pred_pym_svc = svc_model.predict(X_test_pym_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7935555555555556"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_pym_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_punct(items): # удалим все лишние символы\n",
    "    z = np.zeros_like(items)\n",
    "    for i in tqdm(range(len(items))):\n",
    "        sentence_list = [\n",
    "             word for word in items[i].split()\n",
    "             if (word not in stopwords.words('russian')) and (word not in punctuation)\n",
    "        ]\n",
    "        z[i] = ' '.join(sentence_list)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 21000/21000 [06:03<00:00, 57.84it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 9000/9000 [02:39<00:00, 56.51it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_p = delete_punct(X_train_pym)\n",
    "X_test_p = delete_punct(X_test_pym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer()\n",
    "X_train_p_bow = vec.fit_transform(X_train_p)\n",
    "X_test_p_bow = vec.transform(X_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.fit(X_train_p_bow, y_train)\n",
    "y_pred_p_lr = lr_model.predict(X_test_p_bow) # Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8067777777777778"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_p_lr) # как видим, удаление шумов не особо повышает долю правильных ответов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ВЫВОД: использование pymorphy достаточно сильно улучшает качество работы алгоритма, однако добавление к предобработке стоп-слов не особо меняет погоду, но хотя бы не ухудшает качество на этих данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Попробуем пробить 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer # сделаем заход через \n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X_train_tf = tfidf_vec.fit_transform(X_train_p)\n",
    "X_test_tf = tfidf_vec.transform(X_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.fit(sc.fit_transform(X_train_tf), y_train)\n",
    "y_pred_tf_lr = lr_model.predict(sc.fit_transform(X_test_tf)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.802"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_tf_lr) # не помогло..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashing Vectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21000, 1024)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = HashingVectorizer(n_features=2**10)\n",
    "X_train_hash = vectorizer.fit_transform(X_train_pym)\n",
    "X_train_hash.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 1024)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_hash = vectorizer.transform(X_test_pym)\n",
    "X_test_hash.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lr_model.fit(X_train_hash, y_train)\n",
    "y_pred_hash_lr = lr_model.predict(X_test_hash) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6896666666666667"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_hash_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = svc_model.fit(X_train_hash, y_train)\n",
    "y_pred_hash_svc = svc_model.predict(X_test_hash) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.727"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_hash_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "X_clf = Pipeline([('vect', HashingVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', LinearSVC(random_state=13)),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clf = text_clf.fit(X_train_p, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_pipe = text_clf.predict(X_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8364444444444444"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred_pipe, y_test)  # УРА!!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последний метод нашла тут: https://habr.com/ru/post/266025/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
